<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dish-n-Dash - Implementation Details</title>
    <style>
        :root {
            --primary-color: #3b82f6;
            --secondary-color: #dbeafe;
            --text-color: #1f2937;
            --light-gray: #f3f4f6;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
        }

        body {
            line-height: 1.6;
            color: var(--text-color);
        }

        .nav {
            background: white;
            padding: 1rem 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .nav-links {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-links a {
            color: var(--text-color);
            text-decoration: none;
            margin-left: 2rem;
        }

        .logo {
            font-weight: bold;
            font-size: 1.5rem;
            color: var(--primary-color);
            text-decoration: none;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        .section {
            padding: 4rem 0;
        }

        .section-title {
            text-align: center;
            font-size: 2rem;
            margin-bottom: 2rem;
        }

        .subsection {
            margin: 3rem 0;
        }

        .subsection h3 {
            color: var(--primary-color);
            margin-bottom: 1rem;
        }

        .implementation-card {
            background: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin: 2rem 0;
        }

        .image-container {
            margin: 2rem 0;
            text-align: center;
        }

        .tech-image {
            max-width: 80%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .image-caption {
            margin-top: 1rem;
            font-style: italic;
            color: #666;
        }

        .implementation-text {
            margin: 1.5rem 0;
            line-height: 1.8;
            font-size: 1.1rem;
        }

        .pipeline-container {
            margin: 3rem 0;
            padding: 2rem;
            text-align: center;
        }

        .pipeline-diagram {
            width: 100%;
            max-width: 1000px;
            margin: 2rem auto;
            padding: 1rem;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .pipeline-details {
            margin-top: 3rem;
        }

        .pipeline-stage {
            margin-bottom: 2.5rem;
            padding: 1.5rem;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .pipeline-stage h4 {
            color: var(--primary-color);
            font-size: 1.25rem;
            margin-bottom: 1rem;
        }

        @media (max-width: 768px) {
            .nav-links {
                flex-direction: column;
                gap: 1rem;
            }
            .nav-links a {
                margin-left: 0;
            }
        }
    </style>
</head>
<body>
    <nav class="nav">
        <div class="nav-links">
            <a href="index.html" class="logo">Dish-n-Dash</a>
            <div>
                <a href="index.html#about">About</a>
                <a href="index.html#features">Features</a>
                <a href="index.html#team">Team</a>
                <a href="design.html">Design</a>
                <a href="implementation.html">Implementation</a>
                <a href="https://github.com/sakakabota/DishnDash">GitHub</a>
            </div>
        </div>
    </nav>

    <section class="section">
        <div class="container">
            <h2 class="section-title">Technical Implementation</h2>
            
            <div class="subsection">
                <h3>Vision System Implementation</h3>
                <div class="implementation-card">
                    <div class="image-container">
                        <img src="assets/realsense.png" alt="Intel RealSense Camera" class="tech-image">
                        <p class="image-caption">Intel RealSense Depth Camera</p>
                    </div>
                    <p class="implementation-text">
                        Our vision system leverages the powerful combination of an Intel RealSense depth camera and a 
                        Detection Transformer (DETR) model for robust object detection. We deployed our DETR server on 
                        Berkeley's honeydew.ist data center (128.32.176.100:8000), utilizing a ResNet-50 backbone for 
                        efficient feature extraction and reliable object detection.
                    </p>
                    
                    <p class="implementation-text">
                        The RealSense camera provides both RGB imagery and precise depth information, enabling our system 
                        to accurately locate objects in 3D space. We mounted the camera above our workspace, providing 
                        a clear view of the sorting area while avoiding interference with the robot's movements.
                    </p>
            
                    <p class="implementation-text">
                        For object detection, we trained our system on a custom dataset featuring common cafeteria items: 
                        apples, oranges, bananas, and various utensils. Each item was photographed from multiple angles 
                        and positions, creating a robust training set that ensures reliable detection regardless of the 
                        object's orientation.
                    </p>
            
                    <p class="implementation-text">
                        When an image is captured, our client script processes and sends it to the DETR server, which 
                        returns precise bounding box coordinates and object centroids. This information is then transformed 
                        into robot coordinates, enabling accurate picking and sorting movements.
                    </p>
                </div>
            </div>

            <div class="subsection">
                <h3>System Pipeline</h3>
                <div class="implementation-card">
                    <div class="pipeline-diagram">
                        <svg viewBox="0 0 800 200">
                            <rect x="20" y="30" width="160" height="80" rx="8" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
                            <rect x="220" y="30" width="160" height="80" rx="8" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
                            <rect x="420" y="30" width="160" height="80" rx="8" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
                            <rect x="620" y="30" width="160" height="80" rx="8" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
                            
                            <path d="M180 70 L220 70" stroke="#3b82f6" stroke-width="2" marker-end="url(#arrowhead)"/>
                            <path d="M380 70 L420 70" stroke="#3b82f6" stroke-width="2" marker-end="url(#arrowhead)"/>
                            <path d="M580 70 L620 70" stroke="#3b82f6" stroke-width="2" marker-end="url(#arrowhead)"/>
                            
                            <defs>
                                <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                    <polygon points="0 0, 10 3.5, 0 7" fill="#3b82f6"/>
                                </marker>
                            </defs>
                            
                            <text x="100" y="75" text-anchor="middle" font-family="system-ui" font-size="16" fill="#1f2937">Vision Processing</text>
                            <text x="300" y="75" text-anchor="middle" font-family="system-ui" font-size="16" fill="#1f2937">Coordinate Transform</text>
                            <text x="500" y="75" text-anchor="middle" font-family="system-ui" font-size="16" fill="#1f2937">Motion Planning</text>
                            <text x="700" y="75" text-anchor="middle" font-family="system-ui" font-size="16" fill="#1f2937">Execution</text>
                        </svg>
                    </div>
                    
                    <div class="pipeline-details">
                        <div class="pipeline-stage">
                            <h4>Vision Processing</h4>
                            <p class="implementation-text">
                                Our vision system begins with image capture through the Intel RealSense camera, feeding 
                                directly into our DETR-based detection model running on Berkeley's servers. This stage 
                                processes real-time imagery to identify bowls, food items, and utensils with high precision, 
                                providing crucial object classification and location data for downstream processing.
                            </p>
                        </div>
            
                        <div class="pipeline-stage">
                            <h4>Coordinate Transform</h4>
                            <p class="implementation-text">
                                The system then transforms detected object coordinates from camera space to robot space 
                                through a carefully calibrated transformation matrix. This crucial step ensures accurate 
                                spatial awareness, enabling precise object manipulation by converting 2D camera detections 
                                into 3D coordinates that the robot can understand and act upon.
                            </p>
                        </div>
            
                        <div class="pipeline-stage">
                            <h4>Motion Planning</h4>
                            <p class="implementation-text">
                                With transformed coordinates in hand, our motion planning system calculates optimal 
                                trajectories for the Sawyer robot. This includes planning pick-up movements for bowls, 
                                determining appropriate tilting angles for content disposal, and ensuring collision-free 
                                paths to the correct disposal locations based on content classification.
                            </p>
                        </div>
            
                        <div class="pipeline-stage">
                            <h4>Execution</h4>
                            <p class="implementation-text">
                                The final stage executes the planned motions with precise control. The system monitors 
                                each movement in real-time, ensuring safe and accurate completion of the sorting task. 
                                This includes bowl grasping, content disposal into appropriate bins (compost for food, 
                                dishes for utensils), and returning to the ready position for the next item.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
</body>
</html>
